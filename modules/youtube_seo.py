#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import os
import random
import re
import time
from datetime import datetime, timedelta
import logging
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class YouTubeSEO:
    """ÙØ¦Ø© Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† Ø³ÙŠÙˆ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨"""
    
    def __init__(self, api_key=None):
        """ØªÙ‡ÙŠØ¦Ø© ÙØ¦Ø© ØªØ­Ø³ÙŠÙ† Ø³ÙŠÙˆ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨"""
        self.api_key = api_key
        self.base_url = "https://www.googleapis.com/youtube/v3"
        self.search_url = "https://www.youtube.com/results"
    
    def analyze_keywords(self, topic, language="ar", use_api=False):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†"""
        if use_api and self.api_key:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            return self._analyze_keywords_api(topic, language)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            return self._simulate_keyword_data(topic, language)
    
    def _simulate_keyword_data(self, topic, language="ar"):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"""
        # Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
        base_keywords = {
            "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³ÙŠÙˆ": [
                "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«", "Ø³ÙŠÙˆ Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†", "ØªØ¹Ù„Ù… Ø§Ù„Ø³ÙŠÙˆ", 
                "ØªØ­Ø³ÙŠÙ† Ø¸Ù‡ÙˆØ± Ø§Ù„Ù…ÙˆÙ‚Ø¹", "ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙÙŠ Ø¬ÙˆØ¬Ù„"
            ],
            "Ø¨Ø±Ù…Ø¬Ø©": [
                "ØªØ¹Ù„Ù… Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©", "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†", "ØªØ¹Ù„Ù… Ø¬Ø§ÙØ§ Ø³ÙƒØ±ÙŠØ¨Øª",
                "Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨", "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¨Ù„ØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ†"
            ],
            "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": [
                "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ³ÙˆÙŠÙ‚", "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø¹Ø¨Ø± Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„", "ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„",
                "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰", "Ø¬Ø°Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙ…Ù„ÙŠÙ†"
            ]
        }
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¹Ø¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ Ù†Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…Ø©
        keywords = []
        if topic in base_keywords:
            keywords = base_keywords[topic]
        else:
            # ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø¯Ø®Ù„
            keywords = [
                f"{topic} Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†", f"ØªØ¹Ù„Ù… {topic}", f"Ø¯Ù„ÙŠÙ„ {topic}",
                f"Ø£Ø³Ø§Ø³ÙŠØ§Øª {topic}", f"ÙƒÙŠÙÙŠØ© {topic}", f"Ø´Ø±Ø­ {topic}",
                f"{topic} Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©", f"Ù†ØµØ§Ø¦Ø­ {topic}", f"Ø£ÙØ¶Ù„ {topic}"
            ]
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ†Ø§ÙØ³ÙŠØ© ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„Ø¨Ø­Ø«
        keyword_data = []
        for kw in keywords:
            # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
            search_volume = random.randint(1000, 50000)
            competition = round(random.uniform(0.1, 0.9), 2)
            cpc = round(random.uniform(0.5, 5.0), 2)
            trend = random.choice(["up", "down", "stable"])
            
            keyword_data.append({
                "keyword": kw,
                "search_volume": search_volume,
                "competition": competition,
                "cpc": cpc,
                "trend": trend,
                "score": round((search_volume / 1000) * (1 - competition) * 10, 1)
            })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø©
        keyword_data.sort(key=lambda x: x["score"], reverse=True)
        
        return {
            "topic": topic,
            "language": language,
            "keywords": keyword_data,
            "related_topics": ["ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ©", "Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ", "Ø¯Ø±ÙˆØ³ Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†"]
        }
    
    def analyze_video_ranking(self, video_url, keywords, use_api=False):
        """ØªØ­Ù„ÙŠÙ„ ØªØµÙ†ÙŠÙ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ÙŠÙ† Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©"""
        video_id = self._extract_video_id(video_url)
        if not video_id:
            return {"error": "Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± ØµØ§Ù„Ø­"}
        
        if use_api and self.api_key:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube API Ù„ØªØ­Ù„ÙŠÙ„ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            return self._analyze_ranking_api(video_id, keywords)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            return self._simulate_ranking_data(video_id, keywords)
    
    def _extract_video_id(self, video_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø±Ø§Ø¨Ø· ÙŠÙˆØªÙŠÙˆØ¨"""
        pattern = r'(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
        match = re.search(pattern, video_url)
        return match.group(1) if match else None
    
    def _simulate_ranking_data(self, video_id, keywords):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"""
        video_title = "Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"
        channel_name = "Ø§Ø³Ù… Ø§Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ØµÙ‹Ø§
        if isinstance(keywords, str):
            keywords_list = [k.strip() for k in keywords.split("\n") if k.strip()]
        elif isinstance(keywords, list):
            keywords_list = keywords
        else:
            keywords_list = []
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ ÙˆÙ‡Ù…ÙŠØ© Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©
        keyword_rankings = []
        for keyword in keywords_list:
            if not keyword.strip():
                continue
                
            # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
            position = random.randint(1, 100)
            position_30d_ago = position + random.randint(-20, 20)
            position_change = position_30d_ago - position
            
            keyword_rankings.append({
                "keyword": keyword.strip(),
                "position": position,
                "position_30d_ago": position_30d_ago,
                "position_change": position_change,
                "search_volume": random.randint(500, 20000),
                "in_title": keyword.lower() in video_title.lower(),
                "in_description": random.choice([True, False]),
                "top_competitors": [
                    {"title": f"ÙÙŠØ¯ÙŠÙˆ Ù…Ù†Ø§ÙØ³ 1 Ù„Ù€ {keyword}", "position": random.randint(1, 5)},
                    {"title": f"ÙÙŠØ¯ÙŠÙˆ Ù…Ù†Ø§ÙØ³ 2 Ù„Ù€ {keyword}", "position": random.randint(1, 10)}
                ]
            })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ØªØ±ØªÙŠØ¨
        keyword_rankings.sort(key=lambda x: x["position"])
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
        video_data = {
            "video_id": video_id,
            "title": video_title,
            "channel": channel_name,
            "views": random.randint(1000, 1000000),
            "likes": random.randint(100, 10000),
            "comments": random.randint(10, 1000),
            "keywords": keyword_rankings,
            "estimated_traffic": sum(k["search_volume"] // k["position"] for k in keyword_rankings if k["position"] > 0),
            "seo_score": random.randint(50, 95),
            "improvement_tips": [
                "ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†",
                "ÙƒØªØ§Ø¨Ø© ÙˆØµÙ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ ÙŠØªØ¶Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©",
                "Ø¥Ø¶Ø§ÙØ© ÙˆØ³ÙˆÙ… (tags) Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ",
                "Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ±Ø© Ù…ØµØºØ±Ø© Ù…Ø®ØµØµØ© ÙˆØ¬Ø°Ø§Ø¨Ø©",
                "ØªØ´Ø¬ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚"
            ]
        }
        
        return video_data
    
    def analyze_competitor(self, channel_url, video_count=10, use_api=False):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø§ÙØ³ÙŠ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨"""
        channel_id = self._extract_channel_id(channel_url)
        if not channel_id:
            return {"error": "Ø±Ø§Ø¨Ø· Ø§Ù„Ù‚Ù†Ø§Ø© ØºÙŠØ± ØµØ§Ù„Ø­"}
        
        if use_api and self.api_key:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube API Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³
            return self._analyze_competitor_api(channel_id, video_count)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³
            return self._simulate_competitor_data(channel_id, video_count)
    
    def _extract_channel_id(self, channel_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ù† Ø±Ø§Ø¨Ø· ÙŠÙˆØªÙŠÙˆØ¨"""
        patterns = [
            r'youtube\.com\/channel\/([a-zA-Z0-9_-]+)',
            r'youtube\.com\/c\/([a-zA-Z0-9_-]+)',
            r'youtube\.com\/@([a-zA-Z0-9_-]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, channel_url)
            if match:
                return match.group(1)
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ø³Ù… Ø§Ù„Ù‚Ù†Ø§Ø© ÙƒÙ…Ø¹Ø±Ù
        return channel_url.split('/')[-1]
    
    def _simulate_competitor_data(self, channel_id, video_count):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"""
        channel_name = f"Ù‚Ù†Ø§Ø© {channel_id}"
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
        subscribers = random.randint(1000, 1000000)
        total_views = subscribers * random.randint(5, 50)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
        video_data = []
        for i in range(int(video_count)):
            publish_date = (datetime.now() - timedelta(days=random.randint(1, 365))).strftime("%Y-%m-%d")
            views = random.randint(1000, 100000)
            likes = int(views * random.uniform(0.01, 0.2))
            comments = int(views * random.uniform(0.001, 0.05))
            
            # ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ÙˆÙ‡Ù…ÙŠØ© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
            video_keywords = []
            for j in range(random.randint(3, 8)):
                video_keywords.append(f"ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© {j+1}")
            
            video_data.append({
                "title": f"ÙÙŠØ¯ÙŠÙˆ {i+1}: {' '.join(random.sample(video_keywords, min(3, len(video_keywords))))}",
                "video_id": f"video{i+1}",
                "publish_date": publish_date,
                "views": views,
                "likes": likes,
                "comments": comments,
                "engagement_rate": round((likes + comments) / views * 100, 2),
                "keywords": video_keywords,
                "seo_score": random.randint(50, 95)
            })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª
        video_data.sort(key=lambda x: x["views"], reverse=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        all_keywords = []
        for video in video_data:
            all_keywords.extend(video["keywords"])
        
        # Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©
        keyword_counts = {}
        for keyword in all_keywords:
            if keyword in keyword_counts:
                keyword_counts[keyword] += 1
            else:
                keyword_counts[keyword] = 1
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        top_keywords = [{"keyword": k, "count": v, "percentage": round(v / len(video_data) * 100, 2)} 
                       for k, v in sorted(keyword_counts.items(), key=lambda item: item[1], reverse=True)]
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³
        competitor_data = {
            "channel_id": channel_id,
            "channel_name": channel_name,
            "subscribers": subscribers,
            "total_views": total_views,
            "videos_analyzed": len(video_data),
            "top_videos": video_data[:5],
            "all_videos": video_data,
            "top_keywords": top_keywords[:10],
            "avg_views": int(sum(v["views"] for v in video_data) / len(video_data)),
            "avg_engagement": round(sum(v["engagement_rate"] for v in video_data) / len(video_data), 2),
            "publishing_frequency": random.choice(["ÙŠÙˆÙ…ÙŠ", "Ø£Ø³Ø¨ÙˆØ¹ÙŠ", "Ø´Ù‡Ø±ÙŠ"]),
            "seo_insights": [
                "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ØªÙƒØ±Ø± ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†",
                "Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø£Ø·ÙˆÙ„ ØªØ­Ù‚Ù‚ Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø£ÙƒØ«Ø±",
                "Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ± Ù…ØµØºØ±Ø© Ù…Ø®ØµØµØ© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª",
                "Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù…",
                "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù†Ø¸Ù… ÙÙŠ Ø£ÙˆØµØ§Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª"
            ]
        }
        
        return competitor_data
    
    def optimize_video_content(self, topic=None, current_title=None, current_description=None, target_keywords=None, title=None, description=None, keywords=None, target_audience=None):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø¹Ù†ÙˆØ§Ù† ÙˆÙˆØµÙ"""
        # Ø¯Ø¹Ù… ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„Ù Ù…Ù† optimize_video Ùˆ video_content
        title_to_use = title if title is not None else current_title
        description_to_use = description if description is not None else current_description
        keywords_to_use = keywords if keywords is not None else target_keywords
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ØµÙ‹Ø§
        if isinstance(keywords_to_use, str):
            keywords_list = [k.strip() for k in keywords_to_use.split('\n') if k.strip()]
        elif isinstance(keywords_to_use, list):
            keywords_list = keywords_to_use
        else:
            keywords_list = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ø°Ø§ ØªÙ… ØªÙ‚Ø¯ÙŠÙ…Ù‡
        title_analysis = None
        if title_to_use:
            title_analysis = self._analyze_title(title_to_use, keywords_list)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ø°Ø§ ØªÙ… ØªÙ‚Ø¯ÙŠÙ…Ù‡
        description_analysis = None
        if description_to_use:
            description_analysis = self._analyze_description(description_to_use, keywords_list)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø­Ø³Ù†Ø©
        improved_titles = self._generate_improved_titles(topic, keywords_list, title_to_use)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù…Ø­Ø³Ù†
        improved_description = self._generate_improved_description(topic, keywords_list, description_to_use)
        
        return {
            "topic": topic,
            "target_keywords": keywords_list,
            "title_analysis": title_analysis,
            "description_analysis": description_analysis,
            "improved_titles": improved_titles,
            "improved_description": improved_description
        }
    
    def _analyze_title(self, title, target_keywords):
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        # ØªØ­Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        length = len(title)
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords_included = 0
        for kw in target_keywords:
            if kw.lower() in title.lower():
                keywords_included += 1
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù…
        has_numbers = bool(re.search(r'\d', title))
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¬ÙˆØ¯ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹Ø§Ø·ÙÙŠØ©
        emotion_words = [
            "Ø£ÙØ¶Ù„", "Ø±Ø§Ø¦Ø¹", "Ø®Ø·ÙŠØ±", "Ù…Ø°Ù‡Ù„", "Ù„Ø§ ÙŠØµØ¯Ù‚", "Ø­ØµØ±ÙŠ", "Ù…Ù…ÙŠØ²", 
            "Ø³Ø±ÙŠØ¹", "Ø³Ù‡Ù„", "ØµØ§Ø¯Ù…", "Ù…Ø«ÙŠØ±", "Ø­ØµØ±ÙŠ", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙÙˆØ±ÙŠ"
        ]
        has_emotion = any(word in title.lower() for word in emotion_words)
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†
        improvement_score = 0
        if length <= 60:
            improvement_score += 25
        elif length <= 70:
            improvement_score += 15
        else:
            improvement_score += 5
        
        improvement_score += min(keywords_included * 15, 30)
        
        if has_numbers:
            improvement_score += 15
        
        if has_emotion:
            improvement_score += 15
        
        improvement_score = min(improvement_score, 100)
        
        return {
            "length": length,
            "keywords_included": keywords_included,
            "has_numbers": has_numbers,
            "has_emotion": has_emotion,
            "improvement_score": improvement_score
        }
    
    def _analyze_description(self, description, target_keywords):
        """ØªØ­Ù„ÙŠÙ„ ÙˆØµÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        # ØªØ­Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„ÙˆØµÙ
        length = len(description)
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords_included = 0
        for kw in target_keywords:
            if kw.lower() in description.lower():
                keywords_included += 1
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¬ÙˆØ¯ ÙÙ‡Ø±Ø³ Ø²Ù…Ù†ÙŠ
        has_timestamps = bool(re.search(r'\d+:\d+', description))
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¬ÙˆØ¯ Ø±ÙˆØ§Ø¨Ø·
        has_links = bool(re.search(r'https?://', description))
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¬ÙˆØ¯ Ù‡Ø§Ø´ØªØ§Ø¬Ø§Øª
        has_hashtags = bool(re.search(r'#\w+', description))
        
        return {
            "length": length,
            "keywords_included": keywords_included,
            "has_timestamps": has_timestamps,
            "has_links": has_links,
            "has_hashtags": has_hashtags
        }
    
    def _generate_improved_titles(self, topic, target_keywords, current_title=None):
        """ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø­Ø³Ù†Ø© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ"""
        emotion_words = ["Ø£ÙØ¶Ù„", "Ø±Ø§Ø¦Ø¹", "Ø®Ø·ÙŠØ±", "Ù…Ø°Ù‡Ù„", "Ù„Ø§ ÙŠØµØ¯Ù‚", "Ø­ØµØ±ÙŠ", "Ù…Ù…ÙŠØ²"]
        numbers = ["3", "5", "7", "10", "15"]
        
        title_templates = [
            "{emotion} {number} {keyword} Ù„Ù€ {topic} | Ø´Ø±Ø­ Ù…ÙØµÙ„",
            "{number} Ø£Ø³Ø±Ø§Ø± Ù„Ù€ {keyword} Ø³ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ø© {topic} Ù„Ø¯ÙŠÙƒ",
            "Ø¯Ù„ÙŠÙ„ {topic} Ø§Ù„Ø´Ø§Ù…Ù„ | {keyword} Ø¨ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠØ©",
            "ÙƒÙŠÙ {keyword} Ø¨Ø·Ø±ÙŠÙ‚Ø© {emotion} ÙÙŠ {number} Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·",
            "{topic} Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†: {keyword} Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø³Ù‡Ù„Ø©",
            "ØªØ¹Ù„Ù… {keyword} ÙÙŠ {number} Ø¯Ù‚Ø§Ø¦Ù‚ ÙÙ‚Ø· | {topic} Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø³Ø·",
            "{emotion} Ù†ØµØ§Ø¦Ø­ Ù„Ù€ {topic} | {keyword} Ø´Ø±Ø­ ÙƒØ§Ù…Ù„"
        ]
        
        improved_titles = []
        for _ in range(5):  # ØªÙˆÙ„ÙŠØ¯ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ù‚ØªØ±Ø­Ø©
            template = random.choice(title_templates)
            title = template.format(
                emotion=random.choice(emotion_words),
                number=random.choice(numbers),
                keyword=random.choice(target_keywords),
                topic=topic
            )
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² 60 Ø­Ø±ÙÙ‹Ø§
            if len(title) > 60:
                title = title[:57] + "..."
            
            improved_titles.append(title)
        
        return improved_titles
    
    def _generate_improved_description(self, topic, target_keywords, current_description=None):
        """ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù…Ø­Ø³Ù† Ù„Ù„ÙÙŠØ¯ÙŠÙˆ"""
        primary_keyword = target_keywords[0] if target_keywords else topic
        
        # Ø¨Ù†Ø§Ø¡ ÙˆØµÙ Ù…Ø­Ø³Ù† ÙŠØªØ¶Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        description = f"""ğŸ” {topic} | {primary_keyword}

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø³Ù†ØªØ¹Ø±Ù Ø¹Ù„Ù‰ {topic} Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ØŒ ÙˆÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ {primary_keyword} Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©.

ğŸ“Œ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ:
00:00 Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† {topic}
01:30 Ù„Ù…Ø§Ø°Ø§ {primary_keyword} Ù…Ù‡Ù…ØŸ
03:45 Ø®Ø·ÙˆØ§Øª {primary_keyword} Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
07:20 Ù†ØµØ§Ø¦Ø­ ÙˆØ­ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…Ø©
12:10 ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
15:30 Ù…Ù„Ø®Øµ ÙˆØ®ØªØ§Ù…

ğŸ”¥ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªØªØ¹Ù„Ù…Ù‡Ø§:
- ÙƒÙŠÙÙŠØ© {target_keywords[1] if len(target_keywords) > 1 else primary_keyword} Ø¨Ø·Ø±ÙŠÙ‚Ø© ØµØ­ÙŠØ­Ø©
- Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ù„Ù€ {target_keywords[2] if len(target_keywords) > 2 else primary_keyword}
- Ø£Ø¯ÙˆØ§Øª ØªØ³Ø§Ø¹Ø¯Ùƒ ÙÙŠ {target_keywords[3] if len(target_keywords) > 3 else primary_keyword}
- ÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ {topic}

ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ù…ÙÙŠØ¯Ø©:
Ù…ÙˆÙ‚Ø¹Ù†Ø§: https://example.com
Ø¯ÙˆØ±ØªÙ†Ø§ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ©: https://example.com/course
Ù…Ù‚Ø§Ù„Ø§Øª Ù…ÙÙŠØ¯Ø©: https://example.com/blog

ğŸ“± ØªØ§Ø¨Ø¹ÙˆÙ†Ø§ Ø¹Ù„Ù‰:
Ø§Ù†Ø³ØªØºØ±Ø§Ù…: https://instagram.com/example
ØªÙˆÙŠØªØ±: https://twitter.com/example
ÙÙŠØ³Ø¨ÙˆÙƒ: https://facebook.com/example

ğŸ’¬ Ø§ØªØ±Ùƒ ØªØ¹Ù„ÙŠÙ‚Ùƒ Ø£Ø¯Ù†Ø§Ù‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³ØªÙØ³Ø§Ø± Ø­ÙˆÙ„ {topic}. Ù„Ø§ ØªÙ†Ø³Ù‰ Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨ Ø¨Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„Ø§Ø´ØªØ±Ø§Ùƒ ÙÙŠ Ø§Ù„Ù‚Ù†Ø§Ø© Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙŠØ¯!

#{ ''.join(primary_keyword.split()) } #{ ''.join(topic.split()) } #{ ''.join(target_keywords[1].split()) if len(target_keywords) > 1 else '' }"""
        
        return description

    def analyze_channel_performance(self, channel_url, period=30, use_api=False):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ù‚Ù†Ø§Ø© Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨"""
        channel_id = self._extract_channel_id(channel_url)
        if not channel_id:
            return {"error": "Ø±Ø§Ø¨Ø· Ø§Ù„Ù‚Ù†Ø§Ø© ØºÙŠØ± ØµØ§Ù„Ø­"}
        
        if use_api and self.api_key:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube API Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø©
            return self._analyze_channel_api(channel_id, period)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø©
            return self._simulate_channel_performance(channel_id, period)
    
    def _simulate_channel_performance(self, channel_id, period):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø© Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ"""
        channel_name = f"Ù‚Ù†Ø§Ø© {channel_id}"
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù„Ù„Ù‚Ù†Ø§Ø©
        subscribers = random.randint(1000, 1000000)
        subscribers_growth = f"{random.randint(1, 15)}%"
        total_views = subscribers * random.randint(5, 50)
        total_videos = random.randint(10, 500)
        avg_views_per_video = total_views // total_videos
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù…Ùˆ Ø¹Ù„Ù‰ Ù…Ø¯Ø§Ø± Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        growth_data = []
        for i in range(period):
            date = (datetime.now() - timedelta(days=period-i)).strftime("%Y-%m-%d")
            views = random.randint(avg_views_per_video // 4, avg_views_per_video // 2)
            subs = random.randint(5, 100)
            growth_data.append({
                "date": date,
                "views": views,
                "subscribers": subs
            })
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
        video_performance = []
        for i in range(5):  # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€5 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
            video_performance.append({
                "title": f"ÙÙŠØ¯ÙŠÙˆ {i+1} - Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ",
                "views": random.randint(avg_views_per_video // 2, avg_views_per_video * 2),
                "watch_time": random.randint(100, 5000),
                "retention": random.randint(20, 85),
                "ctr": round(random.uniform(1.5, 15.0), 1),
                "engagement_rate": round(random.uniform(1.0, 10.0), 1)
            })
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keyword_analysis = []
        keywords = ["Ù…ØµØ·Ù„Ø­ ØªÙˆØ¶ÙŠØ­ÙŠ 1", "Ù…ØµØ·Ù„Ø­ ØªÙˆØ¶ÙŠØ­ÙŠ 2", "Ù…ØµØ·Ù„Ø­ ØªÙˆØ¶ÙŠØ­ÙŠ 3", "Ù…ØµØ·Ù„Ø­ ØªÙˆØ¶ÙŠØ­ÙŠ 4"]
        for keyword in keywords:
            keyword_analysis.append({
                "keyword": keyword,
                "impressions": random.randint(1000, 50000),
                "views": random.randint(100, 5000),
                "ctr": round(random.uniform(1.0, 10.0), 1),
                "avg_view_duration": round(random.uniform(1.0, 10.0), 1),
                "videos_ranking": random.randint(1, 10)
            })
        
        # ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ø³ÙŠÙˆ Ø§Ù„Ù‚Ù†Ø§Ø©
        seo_recommendations = [
            "Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª",
            "ØªØ­Ø³ÙŠÙ† Ø£ÙˆØµØ§Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù„ØªØªØ¶Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙƒØ«Ø± ÙˆØ¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ",
            "Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ± Ù…ØµØºØ±Ø© Ø¬Ø°Ø§Ø¨Ø© ÙˆÙ…ØªÙ†Ø§Ø³Ù‚Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª",
            "Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙŠ ØªØ³ØªÙ‡Ø¯Ù Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
            "Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ø¦Ù… ØªØ´ØºÙŠÙ„ Ù…Ø®ØµØµØ© Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©",
            "ØªØ´Ø¬ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„Ø²ÙŠØ§Ø¯Ø© Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ",
            "Ù†Ø´Ø± Ù…Ø­ØªÙˆÙ‰ Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù… ÙˆÙ…ØªØ³Ù‚ Ù„ØªØ­Ø³ÙŠÙ† ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ†",
            "ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ ÙˆØµÙ Ø§Ù„Ù‚Ù†Ø§Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§ØªÙ‡Ø§",
            "Ø¥Ù†Ø´Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ù…ØªØ¨Ø§Ø¯Ù„Ø© Ø¨ÙŠÙ† Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©"
        ]
        
        # Ø¯Ø±Ø¬Ø© Ø³ÙŠÙˆ Ø§Ù„Ù‚Ù†Ø§Ø©
        seo_score = random.randint(50, 95)
        
        return {
            "channel_name": channel_name,
            "channel_id": channel_id,
            "period": period,
            "seo_score": seo_score,
            "channel_stats": {
                "subscribers": subscribers,
                "subscribers_growth": subscribers_growth,
                "total_views": total_views,
                "total_videos": total_videos,
                "avg_views_per_video": avg_views_per_video
            },
            "growth_data": growth_data,
            "video_performance": video_performance,
            "keyword_analysis": keyword_analysis,
            "seo_recommendations": seo_recommendations
        }
